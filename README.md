# Kafka-Postgres
Data stream processing with Apache Kafka connectors, sinks and PostgreSQL
# Use Case
An ecommerce store with transactional data and wishes to process the event data generated by transactions. 

# Tech used
- Apache Kafka for stream data handling 
- Zookeeper for managing Kafka
- Kafka Connect for "transporting data into and out of Kafka
- Postgres for working with Kafka connectors to handle data going into and out of Kafka
- KSQL server for create real-time processing
- Kafka's schema registry for imposing the AVRO format.

# Connectors
`source.json` (source) Defines the connection between the source database(postgres) and Kafka as the destination
`sink.json` (Sink) Defines the connection between Kafka as the source and a postgres DB as the destination
# Running the project
Spin up the services using the command ` docker-compose up ` use `-d` flag to run in detatch mode.

## Setup a Postgres DB and Load Data
On a separate terminal, To spin up a postgres db, run the command
```
docker run -it --rm --network=kafka_postgres_default \
         -v $PWD:/home/data/ \
         postgres:11.0 psql -h postgres -U postgres
```
In the PSQL interface, run the commands defined in the `tables.sql` file
## Make source connection
Submit `source.json` file to the connect service via a curl command
```
curl -X POST -H “Accept:application/json” -H “Content-Type: application/json” --data @source.json http://localhost:8083/connectors
```
Query the connector to see if it worked `curl -H “Accept:application/json” localhost:8083/connectors/`

if successful, the transactions table should be seen as a TOPIC. Run the command
` docker exec -it kafka /bin/bash` to access the kafka container bash. In this case, "kafka" is the container name. You can also use the container ID
Once in the container bash, run the commmand `/usr/bin/kafka-topics — list — zookeeper zookeeper:2181` to view topics.
