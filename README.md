# Kafka-Postgres
Data stream processing with Apache Kafka connectors, sinks and PostgreSQL
# Use Case
An ecommerce store with transactional data and wishes to process the event data generated by transactions. 

# Tech used
- Apache Kafka for stream data handling 
- Zookeeper for managing Kafka
- Kafka Connect for "transporting data into and out of Kafka
- Postgres for working with Kafka connectors to handle data going into and out of Kafka
- KSQL server for create real-time processing
- Kafka's schema registry for imposing the AVRO format.

# Connectors
`source.json` (source) Defines the connection between the source database(postgres) and Kafka as the destination
`sink.json` (Sink) Defines the connection between Kafka as the source and a postgres DB as the destination